<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quality Prompts</title>
    <link rel="stylesheet" href="./css/styles.css">
    <link rel="stylesheet" href="./css/components.css">
    <link rel="icon" type="image/svg+xml" href="./assets/favicon.svg">
    <script src="https://js.puter.com/v2/"></script>
</head>
<body>
    <h1>Quality Prompts</h1>
    <p class="subtitle">Transform simple ideas into production-ready prompts</p>
    <div class="how-to-use-row">
        <button id="btn-info" class="btn-how-to-use">How to use</button>
    </div>

    <!-- Info Modal -->
    <div id="info-modal" class="modal-overlay hidden">
        <div class="modal">
            <div class="modal-header">
                <h3>About Quality Prompts</h3>
                <button id="btn-close-info" class="btn-modal-close">&times;</button>
            </div>
            <div class="modal-body">
                <p><strong>Quality Prompts</strong> is a free, serverless tool that transforms simple ideas into high-quality, production-ready prompts. You provide a short description of what you want, and the tool builds a detailed, structured prompt for you. It adds objectives, constraints, deliverables, evaluation criteria, and edge case handling automatically.</p>

                <h4>How it works</h4>
                <p>Select a subject type, then optionally choose a prompt style to tailor the output for a specific workflow. Choose a target model class to optimize prompt length and complexity. Type a short idea into the text field, a sentence or two is ideal. Click Generate Prompt and the tool returns your optimized prompt in three formats: Plain Text for copy-pasting, Structured with markdown, and JSON for agents and APIs.</p>

                <h4>Prompt styles</h4>
                <p>Every subject type has specialized prompt styles that adapt the generated prompt for different workflows. When you select a subject type, the Prompt Style dropdown appears with options tailored to that category. Select General to use the default dimensions without a specific style.</p>

                <p><strong>Development</strong> — Specification Prompt for new projects with no prior context. Iteration Prompt for targeted changes to existing code. Diagnostic Prompt for debugging unknown failures.</p>

                <p><strong>Writing</strong> — Creative Writing (Long-Form) for essays, articles, and narrative work. Short-Form Copy for ads, taglines, and microcopy. Marketing Communications for emails, newsletters, and press releases.</p>

                <p><strong>Strategy</strong> — Business Strategy for competitive positioning and growth planning. Go-to-Market Strategy for launch plans and pricing. Technical Strategy for architecture decisions and technology selection.</p>

                <p><strong>Product</strong> — Product Requirements Document for full PRDs with acceptance criteria. User Stories for sprint-ready stories in standard format. Feature Specification for detailed single-feature specs covering all states.</p>

                <p><strong>Design</strong> — UI/UX Design for wireframes, flows, and component specs. Design Assets for logos, icons, and visual elements. Photo Editing for image modification and compositing instructions.</p>

                <p><strong>Marketing</strong> — Campaign Planning for multi-channel campaigns with budgets and KPIs. Content Strategy for editorial calendars and content pillars. Social Media for platform-specific strategies and posts.</p>

                <p><strong>Research</strong> — Literature Review for systematic source synthesis and gap analysis. User Research for interview guides, surveys, and usability studies. Market Research for competitive analysis and market sizing.</p>

                <p><strong>Data Analysis</strong> — Exploratory Analysis for data profiling and pattern discovery. Dashboard &amp; Reporting for KPI dashboards with metric definitions. Statistical Modeling for regression, hypothesis testing, and model validation.</p>

                <h4>API providers</h4>
                <p>Quality Prompts supports multiple API providers. Select your provider in the API Settings panel and enter your API key.</p>
                <p><strong>Puter GPT-OSS</strong> is the default and is completely free with no API key required. It uses Puter's user-pays model, which means you as a developer or site owner pay nothing. Instead, each person who uses the tool covers their own AI inference cost through their Puter account. When a user first triggers an AI request, Puter handles authentication automatically. Users get a free usage allowance, and any usage beyond that is billed to their own Puter account, not yours. This means the tool can scale to unlimited users at zero cost to the host. The gpt-oss-120b model (117B parameters) produces the best results but is slower. The gpt-oss-20b model (21B parameters) is faster with lower quality. Both are open-weight models from OpenAI running on Puter's infrastructure.</p>
                <p><strong>OpenRouter</strong> is recommended for users who want access to hundreds of models (Claude, GPT, Gemini, Llama, Mistral, and more) through a single API key. It is CORS-friendly and works directly in the browser without a proxy. Get a key at openrouter.ai/keys.</p>
                <p><strong>Anthropic</strong> connects directly to the Claude Messages API. Get a key at console.anthropic.com.</p>
                <p><strong>OpenAI</strong> connects to the Chat Completions API. Get a key at platform.openai.com.</p>
                <p><strong>Google Gemini</strong> connects to the Gemini API. Get a key at aistudio.google.com.</p>
                <p><strong>Ollama</strong> runs AI models locally on your machine with gpt-oss:20b as the default model. No API key needed, no data leaves your computer, completely free. Ollama is the recommended fallback if Puter's free tier runs out — Quality Prompts will automatically suggest it with OS-specific setup instructions (macOS, Windows, Linux) if Puter encounters an error. This works from any URL including GitHub Pages — you just need to start Ollama with <code>OLLAMA_ORIGINS=*</code> to allow browser access. Click Setup Instructions in the Ollama settings for a full guide with OS-specific steps. Quality Prompts checks your installed models and verifies the connection before sending the request.</p>
                <p><strong>Custom Endpoint</strong> supports any OpenAI-compatible API including Together, LM Studio, and other self-hosted models. Enter your base URL and it uses the /chat/completions format with Bearer token auth.</p>

                <h4>Tips for best results</h4>
                <p>Keep your idea brief. The whole point of this tool is to do the heavy lifting for you, so you do not need to write a full prompt yourself. Focus on what you want accomplished, not on how to prompt for it. The more specific your idea is, the better the output will be. For example, "build a dashboard that tracks user retention by cohort" will produce a much better prompt than just "build a dashboard."</p>

                <h4>Sharing and using your prompt</h4>
                <p>The <strong>Share Idea</strong> button (above Generate Prompt) opens a modal where you can copy a prefilled URL of your idea. Choose "Copy link" to share a URL that prefills the idea for the recipient, or "Copy link with auto-generate" to share a URL that also triggers prompt generation automatically when opened.</p>

                <p>Once a prompt is generated, the <strong>Use this prompt</strong> section at the bottom lets you open the generated prompt directly in ChatGPT, Claude, Copilot, or Gemini. ChatGPT receives the prompt automatically. For Claude, Copilot, and Gemini, the prompt is copied to your clipboard and the service opens in a new tab — just paste it when the page loads.</p>

                <p>You can also <strong>Share via Email</strong> from the Plain Text tab to send the full generated prompt by email.</p>

                <h4>Output formats</h4>
                <p>The Plain Text tab gives you a clean, readable prompt you can paste directly into any chat interface. The Structured tab formats the same prompt with markdown headings, sections, and bullet points for easy scanning. The JSON tab provides a programmatic format with separate fields for system instructions, user prompts, constraints, and evaluation criteria, which is useful for agents and API integrations.</p>

                <h4>URL routing</h4>
                <p>You can prefill the idea input via URL. Add <code>?prompt=your idea here</code> to the URL to prefill, or add <code>&amp;enter</code> to also auto-generate the prompt on page load. The bare format <code>?=your idea here</code> also works. This is useful for bookmarks, integrations, and the Share Idea feature.</p>

                <p>All processing happens through the selected API provider. No data is stored on any server.</p>
            </div>
        </div>
    </div>

    <div id="file-protocol-warning" class="warning-banner hidden">
        <strong>Local file detected.</strong> The Puter API requires a web server. Run this from your project folder:
        <code>python3 -m http.server 8000</code> then open <a href="http://localhost:8000">http://localhost:8000</a>
    </div>

    <div class="container">
        <!-- Settings Panel -->
        <button id="settings-toggle" class="settings-toggle">&#9654; API Settings</button>
        <div id="settings-panel" class="settings-panel hidden">
            <div class="form-group">
                <label for="api-mode">API Provider</label>
                <select id="api-mode">
                    <option value="puter">Puter GPT-OSS (Free, no key required)</option>
                    <option value="openrouter">OpenRouter (CORS-friendly, many models)</option>
                    <option value="anthropic">Anthropic Claude</option>
                    <option value="openai">OpenAI</option>
                    <option value="google">Google Gemini</option>
                    <option value="ollama">Ollama (Local)</option>
                    <option value="custom">Custom Endpoint</option>
                </select>
            </div>

            <!-- Puter Settings (no key needed) -->
            <div id="puter-settings" class="provider-settings">
                <div class="form-group">
                    <label for="puter-model">Model</label>
                    <select id="puter-model">
                        <option value="openai/gpt-oss-120b">gpt-oss-120b (117B params, best quality)</option>
                        <option value="openai/gpt-oss-120b:exacto">gpt-oss-120b:exacto (precise variant)</option>
                        <option value="openai/gpt-oss-20b">gpt-oss-20b (21B params, faster)</option>
                    </select>
                </div>
                <p class="settings-hint">Powered by Puter.com — free, no API key needed.</p>
            </div>

            <!-- Ollama Settings (local, no key needed) -->
            <div id="ollama-settings" class="provider-settings hidden">
                <div class="form-group">
                    <label for="ollama-url">Ollama URL</label>
                    <input type="text" id="ollama-url" value="http://localhost:11434" placeholder="http://localhost:11434">
                </div>
                <div class="form-group">
                    <label for="ollama-model">Model</label>
                    <input type="text" id="ollama-model" placeholder="gpt-oss:20b" value="gpt-oss:20b">
                </div>
                <div class="ollama-hint-row">
                    <p class="settings-hint">Runs locally on your machine — no API key, no data leaves your computer.</p>
                    <button type="button" id="btn-ollama-help" class="btn-small">Setup Instructions</button>
                </div>
            </div>

            <!-- Ollama Instructions Modal -->
            <div id="ollama-modal" class="modal-overlay hidden">
                <div class="modal">
                    <div class="modal-header">
                        <h3>Running Quality Prompts with Ollama</h3>
                        <button id="btn-close-ollama" class="btn-modal-close">&times;</button>
                    </div>
                    <div class="modal-body">
                        <p>Ollama lets you run AI models locally on your own machine. No API key needed, no data leaves your computer, and it's completely free. This works whether you're running Quality Prompts locally or from <strong>https://97115104.github.io/qualityprompts/</strong> — your browser connects directly to Ollama on your machine.</p>

                        <p class="alert-box"><strong>Browser requirement:</strong> When using Ollama from GitHub Pages (or any HTTPS URL), <strong>Google Chrome is required</strong>. Safari and other WebKit browsers block HTTPS pages from connecting to local HTTP servers. On localhost, any browser works.</p>

                        <div class="os-tabs">
                            <button class="os-tab-btn active" data-os="mac">macOS</button>
                            <button class="os-tab-btn" data-os="win">Windows</button>
                            <button class="os-tab-btn" data-os="linux">Linux</button>
                        </div>

                        <!-- macOS -->
                        <div class="os-panel active" data-os="mac">
                            <h4>1. Install Ollama</h4>
                            <pre class="terminal-block">curl -fsSL https://ollama.com/install.sh | sh</pre>
                            <p>Or download the macOS app from <strong>ollama.com/download</strong>.</p>

                            <h4>2. Check for existing models</h4>
                            <pre class="terminal-block">ollama list</pre>
                            <p>If you see <strong>gpt-oss:20b</strong> or any <strong>gpt-oss</strong> model, skip to step 4. Otherwise continue.</p>

                            <h4>3. Pull the default model</h4>
                            <pre class="terminal-block">ollama pull gpt-oss:20b</pre>

                            <h4>4. Start Ollama with remote access enabled</h4>
                            <p>First, stop any running Ollama instance. If you see <code>address already in use</code> when running <code>ollama serve</code>, Ollama is already running and must be stopped first:</p>
                            <pre class="terminal-block"># If the desktop app is running, click the menu bar icon → Quit Ollama
# Or kill the process:
pkill ollama</pre>
                            <p>Then start Ollama with browser access:</p>
                            <pre class="terminal-block">OLLAMA_ORIGINS=* ollama serve</pre>
                            <p>To make this permanent so you never have to set it manually, add to your <code>~/.zshrc</code> (or <code>~/.bashrc</code>):</p>
                            <pre class="terminal-block">export OLLAMA_ORIGINS=*</pre>
                            <p>Then restart your terminal and the Ollama desktop app will use this setting automatically.</p>
                        </div>

                        <!-- Windows -->
                        <div class="os-panel" data-os="win">
                            <h4>1. Install Ollama</h4>
                            <p>Download the Windows installer from <strong>ollama.com/download</strong> and run it.</p>

                            <h4>2. Check for existing models</h4>
                            <p>Open PowerShell and run:</p>
                            <pre class="terminal-block">ollama list</pre>
                            <p>If you see <strong>gpt-oss:20b</strong> or any <strong>gpt-oss</strong> model, skip to step 4. Otherwise continue.</p>

                            <h4>3. Pull the default model</h4>
                            <pre class="terminal-block">ollama pull gpt-oss:20b</pre>

                            <h4>4. Start Ollama with remote access enabled</h4>
                            <p>First, stop any running Ollama instance. If you see <code>address already in use</code>, Ollama is already running. Close it from the system tray (right-click → Quit), or in PowerShell:</p>
                            <pre class="terminal-block">Stop-Process -Name ollama -Force</pre>
                            <p>Then start with browser access:</p>
                            <pre class="terminal-block">$env:OLLAMA_ORIGINS="*"; ollama serve</pre>
                            <p>To make this permanent so Ollama always allows browser access:</p>
                            <pre class="terminal-block">[System.Environment]::SetEnvironmentVariable("OLLAMA_ORIGINS", "*", "User")</pre>
                            <p>Then restart the Ollama desktop app — it will use this setting automatically.</p>
                        </div>

                        <!-- Linux -->
                        <div class="os-panel" data-os="linux">
                            <h4>1. Install Ollama</h4>
                            <pre class="terminal-block">curl -fsSL https://ollama.com/install.sh | sh</pre>

                            <h4>2. Check for existing models</h4>
                            <pre class="terminal-block">ollama list</pre>
                            <p>If you see <strong>gpt-oss:20b</strong> or any <strong>gpt-oss</strong> model, skip to step 4. Otherwise continue.</p>

                            <h4>3. Pull the default model</h4>
                            <pre class="terminal-block">ollama pull gpt-oss:20b</pre>

                            <h4>4. Start Ollama with remote access enabled</h4>
                            <p>If Ollama is running as a systemd service (most common on Linux), configure the environment variable and restart:</p>
                            <pre class="terminal-block">sudo systemctl edit ollama</pre>
                            <p>Add the following under <code>[Service]</code>:</p>
                            <pre class="terminal-block">[Service]
Environment="OLLAMA_ORIGINS=*"</pre>
                            <p>Then restart the service:</p>
                            <pre class="terminal-block">sudo systemctl restart ollama</pre>
                            <p>If you see <code>address already in use</code> when running manually, stop the service first:</p>
                            <pre class="terminal-block">sudo systemctl stop ollama</pre>
                            <p>Then run directly in your terminal:</p>
                            <pre class="terminal-block">OLLAMA_ORIGINS=* ollama serve</pre>
                        </div>

                        <h4>5. Configure Quality Prompts</h4>
                        <ol>
                            <li>Select <strong>Ollama (Local)</strong> as the API Provider in Settings</li>
                            <li>The URL defaults to <code>http://localhost:11434</code></li>
                            <li>The model defaults to <strong>gpt-oss:20b</strong> — change it if you want to use a different model</li>
                            <li>Click <strong>Generate Prompt</strong> — Quality Prompts will verify the connection and model before sending</li>
                        </ol>

                        <h4>Other recommended models</h4>
                        <ul>
                            <li><strong>gpt-oss:20b</strong> — 21B params, default, same model family as Puter's free tier</li>
                            <li><strong>qwen2.5:14b</strong> — 14B params, excellent for structured output</li>
                            <li><strong>mistral</strong> — 7B params, strong instruction following</li>
                            <li><strong>llama3.2</strong> — 3B params, fastest, good for quick iterations</li>
                        </ul>

                        <h4>Why is OLLAMA_ORIGINS needed?</h4>
                        <p>Browsers enforce a security policy called CORS that blocks web pages from making requests to different servers. When you use Quality Prompts from <code>https://97115104.github.io</code> (or any URL that isn't <code>localhost</code>), your browser needs Ollama to explicitly allow those requests. Setting <code>OLLAMA_ORIGINS=*</code> tells Ollama to accept requests from any web page, including GitHub Pages. This is safe because Ollama is only accessible on your local machine.</p>

                        <h4>Troubleshooting</h4>
                        <p><strong>"address already in use":</strong> Ollama is already running (usually the desktop app). You need to stop it first, then restart with <code>OLLAMA_ORIGINS=*</code>. On macOS: quit from the menu bar icon or run <code>pkill ollama</code>. On Windows: close from the system tray or run <code>Stop-Process -Name ollama -Force</code>. On Linux: <code>sudo systemctl stop ollama</code>. Then run <code>OLLAMA_ORIGINS=* ollama serve</code>.</p>
                        <p><strong>CORS error / Failed to fetch from GitHub Pages:</strong> This means <code>OLLAMA_ORIGINS=*</code> is not set. You must stop Ollama and restart it with the environment variable — it cannot be changed while running. See the instructions for your OS above.</p>
                        <p><strong>Connection refused:</strong> Ollama is not running. Start it with <code>OLLAMA_ORIGINS=* ollama serve</code> and check the URL is <code>http://localhost:11434</code>.</p>
                        <p><strong>Model not found:</strong> Run <code>ollama list</code> to see installed models. Pull the model you need with <code>ollama pull gpt-oss:20b</code>.</p>
                    </div>
                </div>
            </div>

            <!-- Shared fields for all key-based providers -->
            <div id="keyed-settings" class="provider-settings hidden">
                <div class="form-group">
                    <label for="api-key">API Key</label>
                    <input type="password" id="api-key" placeholder="sk-...">
                </div>
                <div id="base-url-group" class="form-group hidden">
                    <label for="base-url">Base URL</label>
                    <input type="text" id="base-url" placeholder="https://api.openai.com/v1">
                </div>
                <div class="form-group">
                    <label for="model-name">Model</label>
                    <input type="text" id="model-name" placeholder="gpt-4o">
                </div>
                <div class="checkbox-group">
                    <input type="checkbox" id="save-key">
                    <label for="save-key">Remember settings in this browser</label>
                </div>
                <p id="provider-hint" class="settings-hint"></p>
            </div>
        </div>

        <hr class="section-divider">

        <!-- Input Section -->
        <div class="form-row">
            <div class="form-group">
                <label for="subject-type">Subject Type</label>
                <select id="subject-type"></select>
            </div>
            <div class="form-group" id="sub-type-group" style="display: none;">
                <label for="sub-type">Prompt Style</label>
                <select id="sub-type">
                    <option value="">General</option>
                </select>
            </div>
            <div class="form-group">
                <label for="model-type">Target Model <span class="info-badge">?
                    <div class="info-tooltip">
                        <strong>Frontier Model</strong> — Detailed prompts (2000+ tokens) with multi-step reasoning, tool-use instructions, and meta-reasoning. For GPT-4o, Claude Opus, Gemini Ultra.<br><br>
                        <strong>LLM (General)</strong> — Balanced prompts (800-1500 tokens) with moderate reasoning depth. For GPT-3.5, Claude Sonnet, Gemini Pro.<br><br>
                        <strong>SLM (Small Model)</strong> — Concise prompts (under 600 tokens) with simple, explicit instructions. For Phi, Mistral 7B, Llama 3B.<br><br>
                        <strong>Paid / Premium</strong> — Extensive prompts leveraging premium capabilities with advanced instruction layering. For ChatGPT Plus, Claude Pro.<br><br>
                        <strong>Open-source Model</strong> — Straightforward prompts (under 800 tokens) with explicit formatting. For Llama, Qwen, local models.
                    </div>
                </span></label>
                <select id="model-type"></select>
            </div>
        </div>

        <div class="form-group">
            <label for="idea-input">Your Idea</label>
            <textarea id="idea-input" maxlength="1500" placeholder="Keep it short, a sentence or two is ideal. The tool builds the full prompt for you." rows="3"></textarea>
            <div class="char-counter"><span id="char-count">0</span><span id="char-limit-display"> / 1,500 characters</span></div>
            <div id="char-limit-hint" class="char-limit-hint hidden">You're at the limit. Remember, the point of this tool is to build the detailed prompt for you — just describe your idea briefly and let Quality Prompts do the rest.</div>
        </div>

        <div id="share-idea-row" class="share-idea-row">
            <button id="btn-share-idea" class="btn-small">Share Idea</button>
        </div>
        <button id="generate-btn" class="btn-primary">Generate Prompt</button>

        <!-- Share Idea Modal -->
        <div id="share-modal" class="modal-overlay hidden">
            <div class="modal modal-small">
                <div class="modal-header">
                    <h3>Share Idea</h3>
                    <button id="btn-close-share" class="btn-modal-close">&times;</button>
                </div>
                <div class="modal-body">
                    <p>Share your idea as a prefilled Quality Prompts link.</p>
                    <div class="share-options">
                        <button id="share-url" class="btn-share-option">Copy link</button>
                        <button id="share-url-enter" class="btn-share-option">Copy link with auto-generate</button>
                    </div>
                    <div id="share-status" class="share-status hidden"></div>
                </div>
            </div>
        </div>

        <!-- Error -->
        <div id="error-section" class="error-message hidden"></div>

        <!-- Loading -->
        <div id="loading-section" class="loading-container hidden">
            <div class="spinner"></div>
            <div id="loading-text" class="loading-text">Generating your prompt...</div>
            <div id="loading-status" class="loading-status">Analyzing subject type, optimizing for target model</div>
            <div id="slow-hint" class="slow-hint hidden">
                Taking too long? <a href="#" id="slow-hint-link">Try a faster model</a>
            </div>
        </div>

        <!-- Speed Tip Modal -->
        <div id="speed-modal" class="modal-overlay hidden">
            <div class="modal modal-small">
                <div class="modal-header">
                    <h3>Switch to a faster model</h3>
                    <button id="btn-close-speed" class="btn-modal-close">&times;</button>
                </div>
                <div class="modal-body">
                    <p>The gpt-oss-120b model (117B parameters) produces the highest quality prompts but can take 30-60 seconds to respond. If speed is more important than quality, switch to the smaller model:</p>
                    <ol>
                        <li>Click <strong>API Settings</strong> at the top of the page</li>
                        <li>Under <strong>Model</strong>, select <strong>gpt-oss-20b (21B params, faster)</strong></li>
                        <li>Click <strong>Generate Prompt</strong> again</li>
                    </ol>
                    <p>The 20b model is roughly 2-3x faster with slightly lower quality. Both models are free through Puter.</p>
                </div>
            </div>
        </div>

        <!-- Puter Fallback Modal -->
        <div id="puter-fallback-modal" class="modal-overlay hidden">
            <div class="modal">
                <div class="modal-header">
                    <h3>Puter isn't available — run locally with Ollama</h3>
                    <button id="btn-close-fallback" class="btn-modal-close">&times;</button>
                </div>
                <div class="modal-body">
                    <p id="puter-fallback-reason"></p>

                    <p>You can run GPT-OSS locally on your machine for free using <strong>Ollama</strong>. No API key, no usage limits, no data leaves your computer. This works from any URL including GitHub Pages.</p>

                    <p class="alert-box"><strong>Browser requirement:</strong> When using Ollama from GitHub Pages (or any HTTPS URL), <strong>Google Chrome is required</strong>. Safari and other WebKit browsers block HTTPS pages from connecting to local HTTP servers.</p>

                    <div class="os-tabs">
                        <button class="os-tab-btn active" data-os="mac">macOS</button>
                        <button class="os-tab-btn" data-os="win">Windows</button>
                        <button class="os-tab-btn" data-os="linux">Linux</button>
                    </div>

                    <!-- macOS -->
                    <div class="os-panel active" data-os="mac">
                        <p><strong>1.</strong> Install Ollama:</p>
                        <pre class="terminal-block">curl -fsSL https://ollama.com/install.sh | sh</pre>

                        <p><strong>2.</strong> Check for existing models and pull GPT-OSS:</p>
                        <pre class="terminal-block">ollama list
ollama pull gpt-oss:20b</pre>

                        <p><strong>3.</strong> Stop the running Ollama instance, then restart with browser access:</p>
                        <pre class="terminal-block"># Quit from menu bar icon, or:
pkill ollama
OLLAMA_ORIGINS=* ollama serve</pre>
                        <p>To make permanent, add <code>export OLLAMA_ORIGINS=*</code> to your <code>~/.zshrc</code> and restart the desktop app.</p>
                    </div>

                    <!-- Windows -->
                    <div class="os-panel" data-os="win">
                        <p><strong>1.</strong> Download and install from <strong>ollama.com/download</strong></p>

                        <p><strong>2.</strong> Open PowerShell, check for models and pull GPT-OSS:</p>
                        <pre class="terminal-block">ollama list
ollama pull gpt-oss:20b</pre>

                        <p><strong>3.</strong> Stop the running Ollama instance, then restart with browser access:</p>
                        <pre class="terminal-block"># Close from system tray, or:
Stop-Process -Name ollama -Force
$env:OLLAMA_ORIGINS="*"; ollama serve</pre>
                        <p>To make permanent: <code>[System.Environment]::SetEnvironmentVariable("OLLAMA_ORIGINS", "*", "User")</code> then restart the app.</p>
                    </div>

                    <!-- Linux -->
                    <div class="os-panel" data-os="linux">
                        <p><strong>1.</strong> Install Ollama:</p>
                        <pre class="terminal-block">curl -fsSL https://ollama.com/install.sh | sh</pre>

                        <p><strong>2.</strong> Check for existing models and pull GPT-OSS:</p>
                        <pre class="terminal-block">ollama list
ollama pull gpt-oss:20b</pre>

                        <p><strong>3.</strong> If running as a service, configure and restart:</p>
                        <pre class="terminal-block">sudo systemctl edit ollama
# Add: Environment="OLLAMA_ORIGINS=*"
sudo systemctl restart ollama</pre>
                        <p>Or stop the service and run manually:</p>
                        <pre class="terminal-block">sudo systemctl stop ollama
OLLAMA_ORIGINS=* ollama serve</pre>
                    </div>

                    <p><strong>4.</strong> Click the button below — it sets the provider to Ollama with the right defaults:</p>

                    <button id="btn-switch-ollama" class="btn-primary" style="margin-top: 8px;">Switch to Ollama now</button>
                </div>
            </div>
        </div>

        <!-- Output -->
        <div id="output-section" class="output-section hidden">
            <hr class="section-divider">
            <h3>Generated Prompt</h3>

            <div class="tab-bar">
                <button class="tab-btn active" data-tab="plain">Plain Text</button>
                <button class="tab-btn" data-tab="structured">Structured</button>
                <button class="tab-btn" data-tab="json">JSON</button>
            </div>

            <!-- Plain Text Tab -->
            <div class="tab-panel active" data-tab="plain">
                <div id="plain-content" class="plain-text-output"></div>
                <div class="btn-row">
                    <button class="btn-small btn-copy" data-target="plain-content">Copy</button>
                    <button class="btn-small btn-download" data-target="plain-content" data-format="txt">Download .txt</button>
                    <button class="btn-small" id="btn-share-email">Share via Email</button>
                </div>
            </div>

            <!-- Structured Tab -->
            <div class="tab-panel" data-tab="structured">
                <pre id="structured-content"></pre>
                <div class="btn-row">
                    <button class="btn-small btn-copy" data-target="structured-content">Copy</button>
                    <button class="btn-small btn-download" data-target="structured-content" data-format="txt">Download .txt</button>
                </div>
            </div>

            <!-- JSON Tab -->
            <div class="tab-panel" data-tab="json">
                <pre id="json-content"></pre>
                <div class="btn-row">
                    <button class="btn-small btn-copy" data-target="json-content">Copy</button>
                    <button class="btn-small btn-download" data-target="json-content" data-format="json">Download .json</button>
                </div>
            </div>

            <!-- Notes -->
            <div id="optimization-notes" class="optimization-notes hidden">
                <strong>Optimization Notes</strong>
                <span></span>
            </div>

            <div id="token-estimate" class="token-estimate hidden"></div>

            <!-- Open in model buttons -->
            <div class="open-in-section">
                <h4>Use this prompt</h4>
                <div class="open-in-row">
                    <button class="btn-open-in btn-chatgpt" data-service="chatgpt">Open in ChatGPT</button>
                    <button class="btn-open-in btn-claude" data-service="claude">Open in Claude</button>
                    <button class="btn-open-in btn-copilot" data-service="copilot">Open in Copilot</button>
                    <button class="btn-open-in btn-gemini" data-service="gemini">Open in Gemini</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Clipboard toast modal -->
    <div id="clipboard-modal" class="clipboard-modal hidden">
        <div class="clipboard-modal-content">
            <button id="clipboard-modal-close" class="btn-modal-close">&times;</button>
            <p id="clipboard-modal-text"></p>
            <div class="clipboard-modal-actions">
                <button id="clipboard-modal-confirm" class="btn-primary">Open now</button>
                <button id="clipboard-modal-cancel" class="btn-small">Cancel</button>
            </div>
        </div>
    </div>

    <footer class="license-section">
        <div class="footer-links">
            <div class="license-badge">
                MIT License
                <div class="license-tooltip">
                    MIT License<br><br>
                    Copyright (c) 2026 Austin Harshberger<br><br>
                    Permission is hereby granted, free of charge, to any person obtaining a copy
                    of this software and associated documentation files (the "Software"), to deal
                    in the Software without restriction, including without limitation the rights
                    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
                    copies of the Software, and to permit persons to whom the Software is
                    furnished to do so, subject to the following conditions:<br><br>
                    The above copyright notice and this permission notice shall be included in all
                    copies or substantial portions of the Software.
                </div>
            </div>
            <a href="https://attest.ink/verify/?data=eyJ2ZXJzaW9uIjoiMi4wIiwiaWQiOiIyMDI2LTAyLTE1LXF1YWxpdHlwcm9tcHRzIiwiY29udGVudF9uYW1lIjoiUXVhbGl0eSBQcm9tcHRzIiwidGltZXN0YW1wIjoiMjAyNi0wMi0xNVQwMDowMDowMC4wMDBaIiwicGxhdGZvcm0iOiJhdHRlc3QuaW5rIiwibW9kZWwiOiJjbGF1ZGUtb3B1cy00LTYiLCJyb2xlIjoiZ2VuZXJhdGVkIiwiZG9jdW1lbnRfdHlwZSI6ImNvZGUiLCJhdXRob3IiOiI5NyAxMTUgMTA0In0=" class="attest-badge" target="_blank" rel="noopener noreferrer">
                built with ai
            </a>
        </div>
        <div class="author-info">
            Created by Austin Harshberger
        </div>
    </footer>

    <script src="./js/promptEngine.js"></script>
    <script src="./js/apiClient.js"></script>
    <script src="./js/uiRenderer.js"></script>
    <script src="./js/app.js"></script>
</body>
</html>
